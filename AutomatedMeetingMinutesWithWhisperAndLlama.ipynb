{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Use Audio to generate meeting minutes, including actions."
      ],
      "metadata": {
        "id": "kQPSM30Q_KA1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "wyR8euYc-o05"
      },
      "outputs": [],
      "source": [
        "! pip install -q requests torch bitsandbytes transformers sentencepiece accelerate  openai httpx==0.27.2 gradio"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#imports\n",
        "\n",
        "import os\n",
        "import requests\n",
        "from IPython.display import Markdown, display, update_display\n",
        "from openai import OpenAI\n",
        "from google.colab import drive\n",
        "from huggingface_hub import login\n",
        "from google.colab import userdata\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, TextStreamer, BitsAndBytesConfig\n",
        "import torch\n",
        "import gradio as gr"
      ],
      "metadata": {
        "id": "XxzmY0AU_m7-"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Models\n",
        "Audio_Model = \"whisper-1\"\n",
        "LLAMA = \"meta-llama/Meta-Llama-3.1-8B-Instruct\""
      ],
      "metadata": {
        "id": "fPH-6ep4Abll"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sign to HuggingFace Hub\n",
        "\n",
        "hf_token = userdata.get('HF_TOKEN')\n",
        "login(hf_token, add_to_git_credential=True)"
      ],
      "metadata": {
        "id": "euyWHVmYA5Rz"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sign in to OpenAI\n",
        "\n",
        "open_api_key = userdata.get('OPENAI_API_KEY')\n",
        "openai = OpenAI(api_key=open_api_key)"
      ],
      "metadata": {
        "id": "Qmw2eqsdCP_b"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#4Bits Quantization\n",
        "\n",
        "quant_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "    bnb_4bit_quant_type=\"nf4\"\n",
        ")"
      ],
      "metadata": {
        "id": "ZejvN1-7HNxb"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def custom_transcription(audio_file):\n",
        "    try:\n",
        "        # The audio_file is already a file path from Gradio\n",
        "        with open(audio_file, \"rb\") as audio_file_obj:\n",
        "            transcription = openai.audio.transcriptions.create(\n",
        "                model=Audio_Model,\n",
        "                file=audio_file_obj,\n",
        "                response_format=\"text\"\n",
        "            )\n",
        "        return transcription\n",
        "    except Exception as e:\n",
        "        return f\"Error in transcription: {str(e)}\""
      ],
      "metadata": {
        "id": "VZiLVVIM3_93"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def extract_notes(transcription):\n",
        "    \"\"\"\n",
        "    Extract notes from transcription using LLAMA model.\n",
        "    Args:\n",
        "        transcription (str): Transcribed text.\n",
        "    Returns:\n",
        "        str: Extracted notes.\n",
        "    \"\"\"\n",
        "    # Define the system and user messages\n",
        "    system_message = \"You are an assistant that produces minutes of meetings from transcripts, with summary, key discussion points, takeaways and action items with owners, in markdown.\"\n",
        "    user_message = f\"Below is an extract transcript of a Denver council meeting. Please write minutes in Markdown, including a summary with attendees, location and date; discussion points; takeaways; and action items with owners.\\n{transcription}\"\n",
        "\n",
        "    # Create the messages list\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": system_message},\n",
        "        {\"role\": \"user\", \"content\": user_message}\n",
        "    ]\n",
        "\n",
        "    # Load the tokenizer for the LLAMA model\n",
        "    tokenizer = AutoTokenizer.from_pretrained(LLAMA)\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "    # Tokenize the input messages using a chat-specific template\n",
        "    inputs = tokenizer.apply_chat_template(messages, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "    # Create a TextStreamer for live text streaming during generation\n",
        "    streamer = TextStreamer(tokenizer)\n",
        "\n",
        "    # Load the LLAMA model for causal language modeling\n",
        "    model = AutoModelForCausalLM.from_pretrained(LLAMA, device_map=\"auto\", quantization_config=quant_config)\n",
        "\n",
        "    # Generate text from the model\n",
        "    outputs = model.generate(inputs, max_new_tokens=2000, streamer=streamer)\n",
        "    notes = tokenizer.decode(outputs[0])\n",
        "\n",
        "    return notes"
      ],
      "metadata": {
        "id": "T5qX9kdE4AGU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def transcribe_and_extract(audio_file):\n",
        "    try:\n",
        "        print(f\"Processing file: {audio_file}\")\n",
        "        transcription = custom_transcription(audio_file)\n",
        "        print(f\"Transcription: {transcription}\")\n",
        "\n",
        "        if \"Error\" in transcription:\n",
        "            return transcription, \"No notes extracted due to transcription error.\"\n",
        "\n",
        "        notes = extract_notes(transcription)\n",
        "        print(f\"Extracted Notes: {notes}\")\n",
        "        return transcription, notes\n",
        "    except Exception as e:\n",
        "        print(f\"Error occurred: {str(e)}\")\n",
        "        return f\"Error: {str(e)}\", \"Error occurred in processing.\""
      ],
      "metadata": {
        "id": "hn4p6DR1cG2K"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "\n",
        "def create_audio_interface():\n",
        "    with gr.Blocks() as demo:\n",
        "        gr.Markdown(\"# Audio Transcription and Notes Extraction\")\n",
        "\n",
        "        # File upload component\n",
        "        audio_input = gr.Audio(type=\"filepath\", label=\"Upload Audio File\")\n",
        "\n",
        "        # Output text areas\n",
        "        transcription_output = gr.Textbox(label=\"Transcription Output\", lines=5)\n",
        "        notes_output = gr.Textbox(label=\"Extracted Notes\", lines=5)\n",
        "\n",
        "        # Buttons\n",
        "        with gr.Row():\n",
        "            transcribe_btn = gr.Button(\"Get Transcription\")\n",
        "            notes_btn = gr.Button(\"Get Notes\")\n",
        "\n",
        "        # Handle transcription button click\n",
        "        def handle_transcription(audio_path):\n",
        "            if audio_path is None:\n",
        "                return \"Please upload an audio file first.\"\n",
        "            transcription, _ = transcribe_and_extract(audio_path)\n",
        "            return transcription\n",
        "\n",
        "        # Handle notes button click\n",
        "        def handle_notes(audio_path):\n",
        "            if audio_path is None:\n",
        "                return \"Please upload an audio file first.\"\n",
        "            _, notes = transcribe_and_extract(audio_path)\n",
        "            return notes\n",
        "\n",
        "        # Connect buttons to functions\n",
        "        transcribe_btn.click(\n",
        "            fn=handle_transcription,\n",
        "            inputs=[audio_input],\n",
        "            outputs=[transcription_output]\n",
        "        )\n",
        "\n",
        "        notes_btn.click(\n",
        "            fn=handle_notes,\n",
        "            inputs=[audio_input],\n",
        "            outputs=[notes_output]\n",
        "        )\n",
        "\n",
        "    return demo\n",
        "\n",
        "# Launch the interface\n",
        "if __name__ == \"__main__\":\n",
        "    demo = create_audio_interface()\n",
        "    demo.launch(share=True)  # share=True for Colab public URL"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 611
        },
        "id": "SWkE0WoWbuDM",
        "outputId": "2cb95704-5ee4-43b4-904c-be04b559667d"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://134a20640ab14d3dbc.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://134a20640ab14d3dbc.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "x4ZE-za5ykBz"
      },
      "execution_count": 14,
      "outputs": []
    }
  ]
}